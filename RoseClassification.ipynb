{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82cbec79-530b-4ac2-b725-b7d2142e01d9",
   "metadata": {},
   "source": [
    "Group 4 \n",
    "Section 46682\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "Hissah Ibn Qurmulah          443200791\n",
    "Reem Alsuhaim\t             443200884\n",
    "Remas Almania\t             443201068\n",
    "----------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260e12a-7830-48f8-9f43-0bc107494376",
   "metadata": {},
   "source": [
    "How to Reproduce Our Results\n",
    "\n",
    "1. Install required libraries:\n",
    "   pip install tensorflow pandas matplotlib seaborn scikit-learn\n",
    "\n",
    "2. Place our dataset inside ./ML_P/new/ folder, structured as:\n",
    "   - new/\n",
    "     - Rose/\n",
    "     - notRose/\n",
    "\n",
    "3. Run Group4_project_code.ipynb \n",
    "\n",
    "\n",
    "4. The final results (accuracy, confusion matrix, etc.) will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccd764-2e89-4bb9-81d4-fe0a527cef42",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea697e-6482-4124-a52c-81a05af21ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, save_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14345023-d408-4070-8b2d-f814ec94630b",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf28e5d-c650-44df-8080-e32bfc69f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data path + path to images after augmentation\n",
    "original_data_dir = 'C:\\\\Users\\\\reems\\\\Desktop\\\\ML_P\\\\new'\n",
    "augmented_data_dir = 'C:\\\\Users\\\\reems\\\\Desktop\\\\ML_P\\\\new_augmented'\n",
    "\n",
    "# Delete the previous folder if it exists + create a new one\n",
    "if os.path.exists(augmented_data_dir):\n",
    "    shutil.rmtree(augmented_data_dir)\n",
    "shutil.copytree(original_data_dir, augmented_data_dir)\n",
    "\n",
    "# Augmented image generation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,          \n",
    "    width_shift_range=0.05,      \n",
    "    height_shift_range=0.05,     \n",
    "    zoom_range=0.05,             \n",
    "    horizontal_flip=True,        \n",
    "    fill_mode='nearest'          \n",
    ")\n",
    "\n",
    "# For each class in the directory , 50% of the images are selected and each one is subjected to one type of prosessing for Augmentation\n",
    "for class_name in os.listdir(original_data_dir):\n",
    "    class_dir = os.path.join(original_data_dir, class_name)\n",
    "    save_dir = os.path.join(augmented_data_dir, class_name)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    images = os.listdir(class_dir)\n",
    "    num_original = len(images)\n",
    "    num_to_generate = int(num_original * 1)  # 50% increase from the original images\n",
    "\n",
    "    selected_images = random.sample(images, num_to_generate)  # Select random images\n",
    "\n",
    "    for idx, img_name in enumerate(selected_images):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = load_img(img_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape) \n",
    "\n",
    "        gen = datagen.flow(\n",
    "            x, \n",
    "            batch_size=1, \n",
    "            save_to_dir=save_dir, \n",
    "            save_prefix='aug', \n",
    "            save_format='jpeg'\n",
    "        )\n",
    "\n",
    "        next(gen)  \n",
    "\n",
    "# show number of image before & after augmentation\n",
    "print(\"number of image before augmentation:\")\n",
    "for class_name in os.listdir(original_data_dir):\n",
    "    class_dir = os.path.join(original_data_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        num_images = len(os.listdir(class_dir))\n",
    "        print(f\"{class_name}: {num_images} image\")\n",
    "\n",
    "print(\"\\nnumber of image after augmentation:\")\n",
    "for class_name in os.listdir(augmented_data_dir):\n",
    "    class_dir = os.path.join(augmented_data_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        num_images = len(os.listdir(class_dir))\n",
    "        print(f\"{class_name}: {num_images} image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ab988-980a-4876-b87a-4a677188ab4d",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba055cf0-11d2-4ae8-8ff8-ca051c36feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new path to store data after splitting\n",
    "train_dir = 'C:\\\\Users\\\\reems\\\\Desktop\\\\ML_P\\\\train'\n",
    "valid_dir = 'C:\\\\Users\\\\reems\\\\Desktop\\\\ML_P\\\\valid'\n",
    "test_dir  = 'C:\\\\Users\\\\reems\\\\Desktop\\\\ML_P\\\\test'\n",
    "\n",
    "# Delete old folders if they exist.\n",
    "for dir_path in [train_dir, valid_dir, test_dir]:\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# split\n",
    "def split_data(original_data_dir, train_dir, valid_dir, test_dir, test_size=0.15, valid_size=0.15):\n",
    "    for class_name in os.listdir(original_data_dir):\n",
    "        class_path = os.path.join(original_data_dir, class_name)\n",
    "        images = os.listdir(class_path)\n",
    "\n",
    "        # split to Train & (Valid + Test)\n",
    "        train_imgs, temp_imgs = train_test_split(images, test_size=(valid_size + test_size), random_state=42)\n",
    "        # split the rest to Valid & Test\n",
    "        valid_imgs, test_imgs = train_test_split(temp_imgs, test_size=test_size / (valid_size + test_size), random_state=42)\n",
    "\n",
    "        # copy image to each folder\n",
    "        for split_imgs, split_dir in zip([train_imgs, valid_imgs, test_imgs], [train_dir, valid_dir, test_dir]):\n",
    "            class_split_dir = os.path.join(split_dir, class_name)\n",
    "            os.makedirs(class_split_dir, exist_ok=True)\n",
    "            for img in split_imgs:\n",
    "                shutil.copy(os.path.join(class_path, img), os.path.join(class_split_dir, img))\n",
    "\n",
    "# Function execution\n",
    "split_data(augmented_data_dir, train_dir, valid_dir, test_dir)\n",
    "\n",
    "# num of iamge in each set + class\n",
    "for name, dir_path in zip(['Train', 'Validation', 'Test'], [train_dir, valid_dir, test_dir]):\n",
    "    print(f\"\\n{name} set:\")\n",
    "    for class_name in os.listdir(dir_path):\n",
    "        class_path = os.path.join(dir_path, class_name)\n",
    "        print(f\"{class_name}: {len(os.listdir(class_path))} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6eabb-3091-4251-a0ac-fefff08a4c88",
   "metadata": {},
   "source": [
    "# Setting ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbed06-3280-449d-90e1-b0a6675802a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Train generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6d962-3d5a-4efc-9686-1f03fbd1c0e0",
   "metadata": {},
   "source": [
    "# Model design + Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98679296-86d1-4ed5-a5ad-d9c9fe714deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3,3), activation='relu', input_shape=(224, 224, 3), kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.005)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.005)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# set learning rate\n",
    "optimizer = Adam(learning_rate=0.0001) \n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', AUC(), Precision(), Recall()]\n",
    ")\n",
    "\n",
    "# EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Evaluation on the validation set\n",
    "val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_generator)\n",
    "\n",
    "# Get the probability\n",
    "val_predictions_prob = model.predict(val_generator, verbose=1)\n",
    "\n",
    "\n",
    "val_labels = val_generator.classes\n",
    "test_labels = test_generator.classes\n",
    "\n",
    "# Get the probability predictions for validation and test sets\n",
    "val_predictions_prob = model.predict(val_generator, verbose=1)\n",
    "test_predictions_prob = model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Convert probabilities into classification (0 or 1) based on threshold\n",
    "threshold = 0.53\n",
    "val_predictions = (val_predictions_prob > threshold).astype(int)\n",
    "test_predictions = (test_predictions_prob > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5985b-e837-4202-b79f-6c49730c8258",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23544370-8e5c-4ab4-8cd8-9dcfd81bb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Evaluation Metrics ======\n",
    "\n",
    "# Calculate Evaluation Metrics for validation set\n",
    "val_predictions = (model.predict(val_generator, verbose=1) > 0.53).astype(int)  # Use the same threshold\n",
    "precision_val = precision_score(val_labels, val_predictions)\n",
    "recall_val = recall_score(val_labels, val_predictions)\n",
    "f1_val = f1_score(val_labels, val_predictions)\n",
    "accuracy_val = accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "# Metrics table for validation set\n",
    "metrics_df_val = pd.DataFrame({\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1-Score\", \"Accuracy\"],\n",
    "    \"Validation Value\": [precision_val, recall_val, f1_val, accuracy_val]\n",
    "})\n",
    "\n",
    "# Calculate Evaluation Metrics for test set\n",
    "test_predictions = (model.predict(test_generator, verbose=1) > 0.53).astype(int)  # Use the same threshold\n",
    "precision_test = precision_score(test_labels, test_predictions)\n",
    "recall_test = recall_score(test_labels, test_predictions)\n",
    "f1_test = f1_score(test_labels, test_predictions)\n",
    "accuracy_test = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Metrics table for test set\n",
    "metrics_df_test = pd.DataFrame({\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1-Score\", \"Accuracy\"],\n",
    "    \"Test Value\": [precision_test, recall_test, f1_test, accuracy_test]\n",
    "})\n",
    "\n",
    "# Merge validation and test metrics\n",
    "metrics_df = pd.merge(metrics_df_val, metrics_df_test, on=\"Metric\")\n",
    "\n",
    "# Draw metrics table for validation and test set\n",
    "plt.figure(figsize=(8, 3))  \n",
    "sns.set(font_scale=1.2) \n",
    "sns.heatmap(metrics_df.set_index(\"Metric\").T, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False, linewidths=1)\n",
    "plt.title(\"Validation vs Test Set Evaluation Metrics\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\")\n",
    "# ====== Confusion matrix ======\n",
    "\n",
    "# Calculate Confusion Matrix for validation set\n",
    "conf_matrix_val = confusion_matrix(val_labels, val_predictions)\n",
    "TN_val, FP_val, FN_val, TP_val = conf_matrix_val.ravel()\n",
    "\n",
    "# Confusion matrix table for validation set\n",
    "conf_matrix_df_val = pd.DataFrame({\n",
    "    \"Predicted Negative\": [TN_val, FN_val],\n",
    "    \"Predicted Positive\": [FP_val, TP_val]\n",
    "}, index=[\"Actual Negative\", \"Actual Positive\"])\n",
    "\n",
    "# Calculate Confusion Matrix for test set\n",
    "conf_matrix_test = confusion_matrix(test_labels, test_predictions)\n",
    "TN_test, FP_test, FN_test, TP_test = conf_matrix_test.ravel()\n",
    "\n",
    "# Confusion matrix table for test set\n",
    "conf_matrix_df_test = pd.DataFrame({\n",
    "    \"Predicted Negative\": [TN_test, FN_test],\n",
    "    \"Predicted Positive\": [FP_test, TP_test]\n",
    "}, index=[\"Actual Negative\", \"Actual Positive\"])\n",
    "\n",
    "# Draw Confusion matrix for validation set\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix_df_val, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, linewidths=1)\n",
    "plt.title(\"Validation Set Confusion Matrix\")\n",
    "\n",
    "# Draw Confusion matrix for test set\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(conf_matrix_df_test, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, linewidths=1)\n",
    "plt.title(\"Test Set Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====== AUC Curve ======\n",
    "\n",
    "# ROC Curve for validation set\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(val_labels, model.predict(val_generator, verbose=1))\n",
    "roc_auc_val = auc(fpr_val, tpr_val)\n",
    "\n",
    "# ROC Curve for test set\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(test_labels, model.predict(test_generator, verbose=1))\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Draw ROC curves for validation and test sets\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr_val, tpr_val, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_val:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--') \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Validation Set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_test, tpr_test, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--') \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Test Set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a190bd2-b175-4cf4-9828-27431d8c4f49",
   "metadata": {},
   "source": [
    "# loss and accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04192df-ddda-4a08-b3e0-c1b58409eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loss and accuracy for test set after training\n",
    "test_loss, test_accuracy, _, _, _ = model.evaluate(test_generator)\n",
    "\n",
    "# Create lists to store test results\n",
    "test_losses = [test_loss] * len(history.history['loss'])\n",
    "test_accuracies = [test_accuracy] * len(history.history['accuracy'])\n",
    "\n",
    "# Draw the loss and accuracy curve during training\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Draw the loss curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='red')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.plot(test_losses, label='Test Loss', color='purple', linestyle='--')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Draw the accuracy curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue')\n",
    "plt.plot(test_accuracies, label='Test Accuracy', color='purple', linestyle='--')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
